---
title: "Bayesian final project"
author: "Yingmai Chen"
date: "2023-12-10"
output: pdf_document
---

```{r echo=FALSE,warning=FALSE,message=FALSE}
library(tidyverse)
library(ggplot2)
library(gridExtra)
library(rstan)
library(bayesplot)
library(rstanarm)
library(shinystan)
library(loo)
```

```{r echo=FALSE}
data<-read.csv("exercise.csv")
data$Gender <- ifelse(data$Gender == "male", 0, 1)
```

# 1 Introduction

The proposed project aims to establish a predictive relationship between physical exercise attributes and calories output.The reason why I choose this project is that:nowadays,the health industry's standard exercise and nutrition advice doesn't fit everyone's unique body responses. Personalized plans are needed for better health outcomes, which requires understanding how individual traits and exercise reactions affect calorie burning.Besides,The study will analyze two datasets: 'exercise.csv' containing variables such as user demographics and post-exercise vitals, and 'calories.csv' detailing corresponding caloric expenditure.

## 1.1 Visualization

For this part, I will show some visualizations of the data.

```{r echo=FALSE, message=FALSE}
plot1 <- ggplot(data, aes(x = as.factor(Gender), y = Calories)) +
         geom_boxplot() +
         scale_x_discrete(labels = c("0" = "Male", "1" = "Female")) +
         labs(title = "Calories Distribution by Gender", x = "Gender", y = "Calories")

plot2 <- ggplot(data, aes(x = Duration, y = Calories)) +
         geom_point(alpha = 0.5) +
         geom_smooth(method = "lm", color = "blue") +
         labs(title = "Duration vs Calories", x = "Duration", y = "Calories")

plot3 <- ggplot(data, aes(x = Body_Temp, y = Calories)) +
         geom_point(alpha = 0.5) +
         geom_smooth(method = "lm", color = "blue") +
         labs(title = "Body Temperature vs Calories", x = "Body Temperature", y = "Calories")

plot4 <- ggplot(data, aes(x = Heart_Rate, y = Calories)) +
         geom_point(alpha = 0.5) +
         geom_smooth(method = "lm", color = "blue") +
         labs(title = "Heart Rate vs Calories", x = "Heart Rate", y = "Calories")

grid.arrange(plot1, plot2, plot3, plot4, ncol = 2, nrow = 2)

```

# 2 Method and analysis

## 2.1 Bayesian linear regression model

### 2.1.1 Method

Inference was conducted using Bayesian inference via Markov Chain Monte Carlo (MCMC) sampling, utilizing the No-U-Turn Sampler (NUTS).

```{r model-fitting, results='hide', message=FALSE, warning=FALSE, echo=FALSE}

data_list <- list(
  N = nrow(data),
  y = data$Calories,
  X = as.matrix(data[, c("Duration", "Heart_Rate", "Body_Temp")]),
  K = 3  
)


stan_model_code <- "
data {
  int<lower=0> N;  // number of observations
  int<lower=0> K;  // number of predictors
  vector[N] y;     // response variable
  matrix[N, K] X;  // predictor matrix
}
parameters {
  vector[K] beta;  // coefficients for predictors
  real<lower=0> sigma;  // standard deviation
}
model {
  y ~ normal(X * beta, sigma);
  beta ~ normal(0, 10);
  sigma ~ inv_gamma(0.01, 0.01);
}
"


fit <- stan(model_code = stan_model_code, data = data_list, iter = 1000, chains = 2)
print(summary(fit))
```

### 2.1.2 Statistical analysis

#### 2.1.2.1 Estimators

The model uses Bayesian estimation, which means the estimators are the posterior distributions of the parameters beta and sigma.

#### 2.1.2.2 Prior

The prior for $\beta$ is a normal distribution with mean 0 and standard deviation 10: $$
\beta \sim \mathcal{N}(0, 10)
$$

The prior for $\sigma$, the standard deviation of the normal distribution for the likelihood, is an inverse gamma distribution with both shape and scale parameters set to 0.01: $$
\sigma \sim \text{Inv-Gamma}(0.01, 0.01)
$$

#### 2.1.2.3 Loss function

The loss function in the context of the Bayesian regression model is the negative log-likelihood of the data given the parameters.

$$
L(\beta, \sigma) = \sum_{i=1}^{N} \frac{1}{2\sigma^2} (y_i - X_i \beta)^2 + \frac{N}{2} \log(2\pi\sigma^2)
$$

In this formula,$L$ is the loss function.$\beta$ is the vector of regression coefficients.$\sigma$ is the standard deviation of the normal distribution. $y_i$ is the observed value of the response variable for the $i$-th observation.$X_i$ is the vector of predictor values for the $i$-th observation.$N$ is the total number of observations.

#### 2.1.2.4 Predictors

The predictors are the variables Duration, Heart Rate, and Body Temperature, which are hypothesized to be associated with the response variable Calories.

### 2.1.3 Sensitive analysis to the prior

```{r echo=FALSE,results='hide',warning=FALSE}
# Adjusted Stan model with different priors
stan_model_code_sensitivity <- "
data {
  int<lower=0> N;  // number of observations
  int<lower=0> K;  // number of predictors
  vector[N] y;     // response variable
  matrix[N, K] X;  // predictor matrix
}
parameters {
  vector[K] beta;  // coefficients for predictors
  real<lower=0> sigma;  // standard deviation
}
model {
  y ~ normal(X * beta, sigma);
  beta ~ normal(0, 1);  // Tighter prior for beta
  sigma ~ inv_gamma(0.1, 0.1);  // Different parameters for sigma
}
"

# Rerun the model with the new priors
fit_sensitivity <- stan(model_code = stan_model_code_sensitivity, data = data_list, iter = 1000, chains = 2)

# Summary of the model with new priors
print(summary(fit_sensitivity))

```

The 95% credible intervals suggest a credible impact of predictors on calories burned and precise estimates of residual standard deviation. Small standard errors indicate high precision in parameter estimates. Convergence diagnostics confirm good model convergence, indicating overall reliability and well-specification of the model.

### 2.1.4 Model checking

```{r echo=FALSE, message=FALSE, fig.height=4, fig.width=8,warning=FALSE}

posterior <- extract(fit)

predicted_y <- apply(posterior$beta, 1, function(beta) data_list$X %*% beta)
mean_predicted_y <- apply(predicted_y, 2, mean)

residuals <- data_list$y - mean_predicted_y

par(mfrow = c(1, 2))
plot(residuals, main = "Residuals Plot", xlab = "Observation", ylab = "Residuals")
abline(h = 0, col = "red")
acf(residuals, main = "ACF of Residuals")
par(mfrow = c(1, 1))


```


The model is free of bias and autocorrelation in residuals but has outliers, indicating some prediction inaccuracies possibly due to extreme values or anomalies.

## 2.2 Hierarchical model based on gender

```{r echo=FALSE,results='hide',warning=FALSE}
data$gender_idx <- as.numeric(factor(data$Gender))

data_list_hierarchical <- list(
  N = nrow(data),
  y = data$Calories,
  X = as.matrix(data[, c("Duration", "Heart_Rate", "Body_Temp")]),
  K = 3,
  J = length(unique(data$gender_idx)), 
  gender = data$gender_idx
)

# Define the Stan model code for the hierarchical model
stan_model_code_hierarchical_2 <- "
data {
  int<lower=0> N;  // number of observations
  int<lower=0> K;  // number of predictors
  int<lower=1> J;  // number of groups (e.g., gender)
  int<lower=1, upper=J> gender[N]; // array to map each observation to a group
  vector[N] y;     // response variable
  matrix[N, K] X;  // predictor matrix
}
parameters {
  vector[K] beta[J];  // array of coefficients for predictors, for each gender group
  real<lower=0> sigma;  // standard deviation
  real<lower=0> sigma_beta; // standard deviation for the distribution of beta across groups
  vector[K] mu_beta;  // mean value of beta coefficients across groups
}
model {
  // Priors
  mu_beta ~ student_t(3, 0, 10); // Student-t priors for mean beta coefficients
  sigma_beta ~ exponential(1); // Exponential prior for beta standard deviation
  
  for (j in 1:J) {
    beta[j] ~ student_t(3, mu_beta, sigma_beta); // Hierarchical Student-t prior for beta
  }

  sigma ~ exponential(1); // Exponential prior for sigma

  // Likelihood
  for (i in 1:N) {
    y[i] ~ student_t(4, X[i] * beta[gender[i]], sigma); // Student-t likelihood
  }
}
"

# Fit the hierarchical model
fit_hierarchical_2 <- stan(
  model_code = stan_model_code_hierarchical_2, 
  data = data_list_hierarchical, 
  iter = 1000, 
  chains = 2
)

# Print the summary of the hierarchical model fit
print(summary(fit_hierarchical_2))
```

### 2.2.1 Method

This is a Bayesian hierarchical model for layered data analysis, using Markov Chain Monte Carlo (MCMC) sampling and posterior analysis via Stan, accommodating complex hierarchical data structures.

### 2.2.2 Statistical analysis

#### 2.2.2.1 Estimators

The estimates include Beta Coefficients (beta[J]),Standard Deviation of Residuals (sigma),standard Deviation of Beta Coefficients Across Groups (sigma_beta),Mean Value of Beta Coefficients Across Groups (mu_beta).

#### 2.2.2.2 Prior

1.  Mean value of beta coefficients across groups (`mu_beta`):$\mu_{\beta} \sim \text{t}(\text{degrees of freedom} = 3, \text{mean} = 0, \text{scale} = 10)$

2.  Standard deviation for the distribution of beta across groups (`sigma_beta`):$\sigma_{\beta} \sim \text{Exponential}(\text{rate} = 1)$

3.  Coefficients for predictors, for each gender group (`beta[j]`):$\beta_{j} \sim \text{t}(\text{degrees of freedom} = 3, \mu_{\beta}, \sigma_{\beta})$

4.  Standard deviation of the residuals (`sigma`):$\sigma \sim \text{Exponential}(\text{rate} = 1)$

#### 2.2.2.3 Loss function

$$
\mathcal{L}(\boldsymbol{\beta}, \sigma) = -\sum_{i=1}^{N} \log \left( \frac{\Gamma\left(\frac{\nu + 1}{2}\right)}{\Gamma\left(\frac{\nu}{2}\right)\sqrt{\nu\pi}\sigma} \left(1 + \frac{1}{\nu} \left(\frac{y_i - \mathbf{X}_i\boldsymbol{\beta}_{\text{gender}[i]}}{\sigma}\right)^2 \right)^{-\frac{\nu + 1}{2}} \right)
$$

From this formula,$N$ is the number of observations.$\mathbf{X}_i$ is the vector of predictor values for the $i$-th observation.$\boldsymbol{\beta}_{\text{gender}[i]}$ is the vector of coefficients corresponding to the gender group of the $i$-th observation.$y_i$ is the actual value of the response variable for the $i$-th observation.$\sigma$ is the scale parameter (standard deviation of the residuals).$\nu$ is the degrees of freedom of the Student-t distribution (set to 4 in the model).$\Gamma$ is the gamma function.

#### 2.2.2.4 Predictors

The predictors are the variables gender, Duration, Heart Rate, and Body Temperature, which are hypothesized to be associated with the response variable Calories

### 2.2.3 Sensitive analysis to the prior

```{r echo=FALSE,results='hide',message=FALSE}
data$gender_idx <- as.numeric(factor(data$Gender))
data_list_hierarchical <- list(
  N = nrow(data),
  y = data$Calories,
  X = as.matrix(data[, c("Duration", "Heart_Rate", "Body_Temp")]),
  K = 3,
  J = length(unique(data$gender_idx)), 
  gender = data$gender_idx
)

# Define the Stan model code for the hierarchical model
stan_model_code_hierarchical_2 <- "
data {
  int<lower=0> N;  // number of observations
  int<lower=0> K;  // number of predictors
  int<lower=1> J;  // number of groups (e.g., gender)
  int<lower=1, upper=J> gender[N]; // array to map each observation to a group
  vector[N] y;     // response variable
  matrix[N, K] X;  // predictor matrix
}
parameters {
  vector[K] beta[J];  // array of coefficients for predictors, for each gender group
  real<lower=0> sigma;  // standard deviation
  real<lower=0> sigma_beta; // standard deviation for the distribution of beta across groups
  vector[K] mu_beta;  // mean value of beta coefficients across groups
}
model {
  // Priors
  mu_beta ~ student_t(3, 0, 10); // Student-t priors for mean beta coefficients
  sigma_beta ~ exponential(1); // Exponential prior for beta standard deviation
  
  for (j in 1:J) {
    beta[j] ~ student_t(3, mu_beta, sigma_beta); // Hierarchical Student-t prior for beta
  }

  sigma ~ exponential(1); // Exponential prior for sigma

  // Likelihood
  for (i in 1:N) {
    y[i] ~ student_t(4, X[i] * beta[gender[i]], sigma); // Student-t likelihood
  }
}
"

# Fit the hierarchical model
fit_hierarchical_2 <- stan(
  model_code = stan_model_code_hierarchical_2, 
  data = data_list_hierarchical, 
  iter = 1000, 
  chains = 2
)

# Print the summary of the hierarchical model fit
print(summary(fit_hierarchical_2))
```

In summary, the model appears to be capturing distinct effects for different gender groups, indicated by the coefficients. However, the relatively large value suggests there is substantial variability not accounted for by the model. It's also important to compare these results with those from a non-hierarchical model to understand the impact of introducing the hierarchical structure.

### 2.2.4 Model checking

```{r echo=FALSE, message=FALSE, fig.height=4, fig.width=8}
posterior_samples <- extract(fit_hierarchical_2)
stan_trace(fit_hierarchical_2, pars = c("beta", "sigma", "sigma_beta", "mu_beta"))
```

It seems that the chains for each parameter are mixing reasonably well. There are no apparent trends or drifts in the chains, which is a good sign of convergence.

# 3.Results and Conclusions

## 3.1 Result of Bayesian linear regression model(Posterior predictive checks)

```{r echo=FALSE, message=FALSE, fig.height=3, fig.width=6}
posterior_samples <- extract(fit)
predicted_y <- matrix(NA, nrow = length(posterior_samples$sigma), ncol = data_list$N)
for (i in 1:length(posterior_samples$sigma)) {
  predicted_y[i, ] <- as.numeric(data_list$X %*% posterior_samples$beta[i, ] + 
                                 rnorm(data_list$N, 0, posterior_samples$sigma[i]))
}
yrep <- apply(predicted_y, 2, mean)
hist(data_list$y, breaks = 40, col = rgb(1, 0, 0, 0.5), main = "Posterior predictive check",
     xlab = "Calories", ylab = "Frequency", xlim = range(c(data_list$y, yrep)))
hist(yrep, breaks = 40, col = rgb(0, 0, 1, 0.5), add = TRUE)
legend("topright", legend = c("Observed", "Predicted"), col = c(rgb(1, 0, 0, 0.5), rgb(0, 0, 1, 0.5)), 
       pch = 15)

```

The model aligns well with observed data for common values and captures the central tendency effectively, as shown by the overlap of distributions and the shape around the histogram's peak. However, there are discrepancies in the tails, particularly the right tail, indicating a possible overestimation of higher values. The model also struggles with extremes and outliers, particularly at the lower end, and may not accurately estimate the likelihood of lower value occurrences. Overall, while the model is generally successful at capturing core data behavior, its accuracy diminishes at the distribution's extremes.

## 3.2 Result of Hierarchical model based on gender(Point estimate)

```{r echo=FALSE,results='hide'}

model_summary <- summary(fit_hierarchical_2)
print(model_summary)
```

```{r echo=FALSE,results='hide'}
mu_beta_mean_1 <- model_summary$summary["mu_beta[1]", "mean"]
mu_beta_mean_2 <- model_summary$summary["mu_beta[2]", "mean"]
mu_beta_mean_3 <- model_summary$summary["mu_beta[3]", "mean"]
sigma_mean <- model_summary$summary["sigma", "mean"]
sigma_beta_mean <- model_summary$summary["sigma_beta", "mean"]

# Create a data frame with these mean values
means_df <- data.frame(
  Parameter = c("mu_beta[1]", "mu_beta[2]", "mu_beta[3]", "sigma", "sigma_beta"),
  MeanValue = c(mu_beta_mean_1, mu_beta_mean_2, mu_beta_mean_3, sigma_mean, sigma_beta_mean)
)

# Now add the standard deviations as you have done in your script
sigma_beta_sd <- model_summary$summary["sigma_beta", "sd"]
sigma_sd <- model_summary$summary["sigma", "sd"]
mu_beta_sd_1 <- model_summary$summary["mu_beta[1]", "sd"]
mu_beta_sd_2 <- model_summary$summary["mu_beta[2]", "sd"]
mu_beta_sd_3 <- model_summary$summary["mu_beta[3]", "sd"]

means_df$StdDev <- c(mu_beta_sd_1, mu_beta_sd_2, mu_beta_sd_3, sigma_sd, sigma_beta_sd)


```

```{r echo=FALSE, message=FALSE, fig.height=2, fig.width=8}
ggplot(means_df, aes(x = Parameter, y = MeanValue, fill = Parameter)) +
  geom_bar(stat = "identity") +
  geom_errorbar(aes(ymin = MeanValue - StdDev, ymax = MeanValue + StdDev), width = 0.2) +
  theme_minimal() +
  labs(title = "Mean Values of Model Parameters with Standard Deviation",
       x = "Parameter",
       y = "Mean Value") +
  scale_fill_brewer(palette = "Pastel1") +
  geom_text(aes(label = round(MeanValue, 2)), vjust = -0.3)
```

The point estimate for mu_beta[1] is precise, evidenced by its small standard deviation. However, mu_beta[2] and especially mu_beta[3] show larger standard deviations, reflecting greater uncertainty in these estimates. The sigma parameter's small point estimate and standard deviation suggest a stable estimation of model residuals. Conversely, the large standard deviation of sigma_beta's point estimate indicates significant uncertainty in estimating group coefficient variability.

## 3.3 Conclusions

In this project, I examined Bayesian regression and hierarchical modeling, emphasizing their efficacy in uncertainty management and prior knowledge integration. These approaches were instrumental in parameter estimation and uncertainty analysis, especially for group variations like gender.This work enhanced my understanding of Bayesian inference, underscoring its relevance in complex data analysis.






